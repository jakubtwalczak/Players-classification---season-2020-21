# -*- coding: utf-8 -*-
"""Klasyfikacja piłkarzy z top 5 lig Europy 20_21.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oEeVNx1R-J8NB9NU8vNota-CpfEtMz3C

W tym projekcie skupię się na zagadnieniu klasyfikacji. Posłuży mi do tego baza danych zbudowana w notatniku dostępnym pod adresem: https://colab.research.google.com/drive/1tYcppW8hd1bCUmyQME2c_Kx6h25IucUN. Baza ta skupia zawodników z pięciu największych lig europejskich: Premier League (Anglia), Serie A (Włochy), Bundesligi (Niemcy), Ligue 1 (Francja) oraz La Liga (Hiszpania) i ich statystyki meczowe za sezon 2020/21. W pierwszej części dokonam klasyfikacji binarnej na napastników i graczy z pozostałych pozycji, w drugiej - postaram się dokonać klasyfikacji wszystkich zawodników wg ich pozycji.

#Import bibliotek.
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report

"""#Załadowanie przygotowanej bazy danych."""

!git clone 'https://gitlab.com/jakub_walczak/players-classification-season-2020-21'
df = pd.read_csv('players-classification-season-2020-21/Top5leagues.csv')

df.head()

"""Już w pierwszych kolumnach uwidacznia się, że baza zawiera znaczną liczbę rekordów NaN, które trzeba w odpowiedni sposób wypełnić. Sprawdźmy więc liczbę nie-NaN-ów w poszczególnych kolumnach za pomocą funkcji pandas.DataFrame.info:"""

df.info()

"""Okazuje się, że liczba NaN-ów jest znaczna, w niektórych kolumnach stanowi nawet niemal wszystkie rekordy. Co więcej, kolumny "Height" i "Weight" zawierają dane typu string (object), ze względu na dodanie na końcu każdego rekordu jednostki miary."""

df.describe().T

"""Powyższe obserwacje potwierdzają również podstawowe statystyki opisowe. Zwróćmy też uwagę, iż dla znacznej części kolumn, w tym kolumny "Apps" przedstawiającej liczbę rozegranych meczów, pierwszy kwartyl przyjmuje wartość 0. Ponieważ chcemy klasyfikator oprzeć głównie na statystykach w poszczególnych elementach gry, trzeba będzie przyjąć pewien próg łącznie rozegranych minut, poniżej których nie klasyfikuję graczy. Tym progiem będzie 270 minut, czyli trzy pełne rozegrane mecze."""

df.columns

"""Przystępujemy do "obróbki" bazy danych. Zacznijmy od przekształcenia danych w kolumnach "Height" i "Weight" na wartości liczbowe typu float poprzez usunięcie jednostki miary i rzutowanie wyniku."""

df['Height'] =  df['Height'].str.replace("\D", "", regex = True).astype(float)
df['Weight'] = df['Weight'].str.replace("\D", "", regex = True).astype(float)

df.isnull().any()

"""Na potrzeby likwidacji wartości NaN przyjmę następującą konwencję:
- wartości w kolumnach dot. statystyk meczowych zastępować będę zerem,
- wartości w kolumnach dot. wzrostu i wagi zastąpię medianą z tych kolumn. 

Da się bowiem przyjąć, że piłkarz może nie zanotować osiągów w jakimś elemencie gry, ale przyjęcie wzrostu lub wagi na poziomie 0 może prowadzić do karykaturalnych wyników.
"""

df['Weight'].fillna((df['Weight'].median()), inplace=True)
df['Height'].fillna((df['Height'].median()), inplace=True)

df.fillna(0, inplace=True)

"""Usuwam kolumnę "Name" (bo nazwiska piłkarzy nam się do niczego nie przydadzą) i "Duels total" (bo została błędnie dodana jako duplikat kolumny "Total duels" przy budowaniu bazy)."""

df.drop(columns=['Name', 'Duels total'], axis = 1, inplace=True)

"""Zgodnie z przyjętym wyżej progiem, usuwam zawodników poniżej 90 rozegranych minut."""

df = df[df['Total minutes']>=270]

df.info()

"""W ten sposób pozbyliśmy się NaN-ów oraz zbędnych kolumn tekstowych, baza danych jest zatem przygotowana do tworzenia modeli. Zgodnie z założeniami jedyną kolumną tekstową jest kolumna "Position", która służyć nam będzie do klasyfikacji graczy."""

df.describe().T

"""Sprawdzamy rozkład zmiennych kategoryzujących dot. pozycji. Zdecydowanie najliczniejsi (i bardzo zbliżeni pod względem liczebności) są obrońcy i pomocnicy, o ok. 1/3 mniej jest napastników, a zdecydowanie najmniejszą grupę stanowią bramkarze (circa pięciokrotnie mniej liczną niż pomocnicy czy obrońcy)."""

df.Position.value_counts(dropna=False, normalize=True)

sns.countplot(x='Position',data=df, palette='GnBu_d')
plt.show()

"""Sprawdźmy teraz statystyki opisowe dla każdej z pozycji, w czym pomoże nam tabela piwotalna.

Separowalność klas dobrze sprawdzić w kolumnach dot. średnich wartości. Najlepiej pod tym względem sprawdzają się zmienne dot. statystyk typowych dla graczy ofensywnych ("Dribble attempts", "Dribble succ.", "Scored goals", "Shots on goal", "Total shots") czy stricte defensywnych ("Blocks", "Interceptions") oraz zmienna dot. całkowitej liczby podań ("Total passes"). 

Statystyki pomocników, z uwagi na swoistą "pośredniość" ustawienia na boisku, bywają zbliżone bądź to do napastników (np. "Assists", "Key passes") bądź obrońców (m.in. "Yellow cards", "Tackles"), bądź jednych i drugich (np. "Duels won"). 

Praktycznie każda kolumna zapewnia separowalność bramkarzy od zawodników z pozostałych pozycji. Kolumny "Apps", "Height", "Weight" nie pozwalają z kolei na dostateczny podział wg etykiet.
"""

pivot = pd.pivot_table(df, index='Position', values = ['Height', 'Weight','Apps', 'Total minutes',
       'Scored goals', 'Assists', 'Total shots', 'Shots on goal',
       'Duels won', 'Dribble attempts', 'Dribbles succ.',
       'Total duels', 'Total passes', 'Key passes', 'Tackles', 'Blocks',
       'Interceptions', 'Total saves', 'Goals conceded', 'Fouls drawn',
       'Fouls committed', 'Yellow cards', 'Red cards for 2nd yellow',
       'Straight red cards', 'Penalties won', 'Penalties scored',
       'Penalties missed', 'Penalties committed', 'Penalties saved'], aggfunc= [np.mean, np.median, min, max, np.std])
pd.options.display.max_columns = None
display(pivot)

"""Heatmap wskazuje wyraźnie, iż znaczna część zmiennych pozostaje w wysokiej korelacji dodatniej. Stwórzmy również funkcję wyświetlającą wszystkie zmienne o korelacji wyższej niż podany próg wg skali Pearsona."""

cm = df.corr()
fig, ax = plt.subplots(figsize=(30,20))
sns.heatmap(cm, annot=True)

def corrFilter(x: pd.DataFrame, y: float):
    xCorr = x.corr()
    xFiltered = xCorr[((xCorr >= y) | (xCorr <= -y)) & (xCorr !=1.000)]
    xFlattened = xFiltered.unstack().sort_values(ascending=False).drop_duplicates().dropna()
    return xFlattened

corrFilter(df, .7)

"""#Napastnicy wśród zawodników z innych pozycji - klasyfikacja binarna.

##Przygotowanie i podział zbiorów, redukcja wymiarów.

Naturalnie rozpocząć powinniśmy od zdefiniowania zbiorów X: zmiennych zależnych (dotyczących statystyk meczowych) i Y: etykiet (pozycji graczy). Zbiory stworzymy bardzo prosto: X - poprzez usunięcie kolumny "Position", Y - poprzez wyodrębnienie kolumny "Position", a następnie poprzez przypisanie tychże do tak nazwanych zmiennych.
"""

X = df.drop(columns='Position')
Y = df['Position']

print(X.head())

Y

"""Na zbiorze Y wykonamy prosty encoding przy pomocy funkcji numpy.where - rekordy dot. napastników zmienimy na 1, dot. innych pozycji - na 0."""

y = np.where(Y == 'Attacker', 1, 0)

y

"""Sprawdźmy teraz, jak rozkładają się zadane klasy."""

np.unique(y, return_counts=True)

from yellowbrick.target import class_balance

class_balance(y.flatten(), labels=['Non-forwards', 'Forwards'])

"""Train test splitem dokonamy podziału na zbiór treningowy (65,7% rekordów) i testowy (33,3%). Parametr stratify pozwoli na zachowanie równego balansu klas w obu zbiorach."""

from sklearn.model_selection import train_test_split, GridSearchCV

X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size = 0.333,
                                                    random_state = 42,
                                                    stratify=y)

print(np.unique(y_train, return_counts=True))
print(np.unique(y_test, return_counts=True))

"""###PCA.

Baza danych, na której pracujemy, zawiera dużą liczbę zmiennych numerycznych (30), w znacznym stopniu skorelowanych. Aby zredukować liczbę zmiennych i korelacji, przydatnym narzędziem będzie PCA - Principal Component Analysis.

Operację z użyciem PCA dokonać należy na danych już zestandaryzowanych. Poszczególne cechy prezentują bowiem różny rząd wysokości, a pożądanym będzie zachowanie równoważności cech.
"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, PolynomialFeatures

sc = StandardScaler()
X_train_sc = sc.fit_transform(X_train)
X_test_sc = sc.transform(X_test)

pca = PCA()

X_train_pca = pca.fit_transform(X_train_sc)
X_test_pca = pca.transform(X_test_sc)

features = range(1, pca.n_components_+1)
plt.bar(features, pca.explained_variance_)
plt.xlim(xmin=0.5)
plt.xlabel('PCA feature')
plt.ylabel('variance')

"""Jak widać, wariancja gwałtownie spada już przy pierwszych 4 cechach. Bardzo wysokie wyjaśnienie wariancji ma jednak z kolei miejsce dopiero przy użyciu mniej więcej 15-17. cechy. Powyżej 20. cechy wyjaśnienie wariancji jest natomiast znikome."""

pca.explained_variance_ratio_

"""Ponownie wytrenujmy zatem PCA, tym razem przy użyciu parametru n_components równego 17."""

pca = PCA(n_components=20)

X_train_sc = pca.fit_transform(X_train_sc)
X_test_sc = pca.transform(X_test_sc)

print(X_train_sc.shape)

features = range(1, pca.n_components_+1)
plt.bar(features, pca.explained_variance_)
plt.xlim(xmin=0.5)
plt.xlabel('PCA feature')
plt.ylabel('variance')

"""Pierwszych 20 cech wyjaśnia ok. 97,3% całości wariancji. Taka liczba zmiennych w zupełności wystarczy do stworzenia modeli zarówno wystarczająco szybkich, ale i dokładnych."""

np.sum(pca.explained_variance_ratio_)

X_train_sc

X_test_sc

"""##Modelowanie.

Na potrzeby modelowania przyjąłem konwencję, wg której hiperparametry do każdego modelu będę dopasowywać przy użyciu zaimportowanej wcześniej funkcji GridSearchCV. Pomoże ona w sprawdzeniu różnych kombinacji parametrów i przeprowadzeniu walidacji krzyżowej. Aby przy tym poszczególne modele działały dosyć szybko, słowniki parametrów nie będą zbyt obszerne i sprowadzać się będą do maksymalnie czterech kluczy.

###Regresja logistyczna.

Regresję logistyczną wytrenujmy, testując od razu różne stopnie wielomianu - pierwszy, drugi, trzeci - oraz parametr C, który możemy określić jako współczynnik kary za błędną klasyfikację. Importujemy funkcje LogisticRegression (dla stworzenia modelu regresji logistycznej) i Pipeline (aby od razu utworzyć sekwencję operacji w modelu).
"""

from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

pipe = Pipeline([
    ('poly', PolynomialFeatures()),
    ('lr', LogisticRegression(warm_start=True, max_iter=50000))
])

params = {'poly__degree': [1, 2, 3], 'lr__C': [0.01, 1, 10, 100]}
gs = GridSearchCV(pipe, param_grid=params, cv=10, verbose=10)
gs.fit(X_train_sc, y_train)
lr_train_pred = gs.predict(X_train_sc)

"""Model sprawdził się najlepiej z pierwszym stopniem wielomianu i przy równej 100 wartości parametru C. Jako najlepszy wynik iteracji GridSearch wskazuje ok. 0.925, czyli całkiem wysoki."""

print(gs.best_params_)
print(gs.best_estimator_)
print(gs.best_score_)

print(classification_report(y_train, lr_train_pred, zero_division=1))

"""Model zanotował bardzo dobre wyniki na zbiorze treningowym, jeśli chodzi o klasyfikację nie-napastników - na 1112 poprawnie przypisał 1070 rekordów. Słabiej poszło natomiast z napastnikami - wartości True Positive i False Negative wynoszą odpowiednio 249 i 59."""

cf_matrix = confusion_matrix(y_train, lr_train_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Other positions', 'Forwards'])

fig, ax = plt.subplots(figsize=(10,10))
cmd.plot(ax=ax)
plt.grid(False)
plt.show()

"""Zbudujmy przy okazji listę wyników dokładności na zbiorze treningowym, którą rozszerzać będziemy przy testowaniu każdego modelu. Będzie to przydatne do podsumowania."""

train_scores = []
train_scores.append(lr_train_pred)

"""Przejdźmy do ewaluacji modelu na zbiorze testowym."""

lr_test_pred = gs.predict(X_test_sc)

"""Ze zbiorem testowym model radzi sobie minimalnie słabiej. Dokładność (accuracy) spadła z 0,93 na 0,92, wszystkie pozostałe wyniki również są niższe o jedną-dwie setne. Model należy jednak ocenić dosyć pozytywnie."""

print(classification_report(y_test, lr_test_pred, zero_division=1))

"""Powyższą obserwację potwierdza matryca pomyłek. Przy zbiorze ponad dwukrotnie mniejszym model odnotowuje o nieco mniej niż połowę liczbę błędnych przypisań."""

cf_matrix = confusion_matrix(y_test, lr_test_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Other positions', 'Forwards'])

fig, ax = plt.subplots(figsize=(10,10))
cmd.plot(ax=ax, cmap='Purples')
plt.grid(False)
plt.show()

"""Analogicznie do zbioru treningowego, zbudujmy listę metryk dokładności dla zbioru testowego."""

test_scores = []
test_scores.append(lr_test_pred)

"""###Maszyny wektorów nośnych (SVM).

####Kernele jednomianowe.

Ponieważ ten model działa szybciej, do SVM dobiorę więcej wartości parametru C. GridSearchCV sprawdzi również, który z kerneli - linearny, gaussowski czy sigmoidalny - będzie klasyfikować najlepiej.
"""

from sklearn.svm import SVC

params = {'C': [0.01, 1, 2, 10, 25, 50, 100], 'kernel': ['linear', 'rbf', 'sigmoid']}
svc = SVC(probability=True)
gs = GridSearchCV(svc, param_grid=params, cv=10, verbose=10)
gs.fit(X_train_sc, y_train)
svm_train_pred = gs.predict(X_train_sc)

"""SVM "domaga się" w tym wypadku tak samo wysokiej wartości współczynnika C. Najlepiej sprawdza się on w połączeniu z kernelem liniowym. Najlepszy wynik również zbliżony jest do 0,925."""

print(gs.best_params_)
print(gs.best_estimator_)
print(gs.best_score_)

"""Wyniki na zbiorze treningowym ponownie są niezłe i bardzo zbliżone do regresji logistycznej. Model popełnia o trzy pomyłki więcej w stosunku do graczy niewystępujących jako napastnicy i tyle samo mniej w stosunku do napastników."""

print(classification_report(y_train, svm_train_pred, zero_division=1))

cf_matrix = confusion_matrix(y_train, svm_train_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Other positions', 'Forwards'])

fig, ax = plt.subplots(figsize=(10,10))
cmd.plot(ax=ax)
plt.grid(False)
plt.show()

train_scores.append(svm_train_pred)

svm_test_pred = gs.predict(X_test_sc)

"""Wyniki na zbiorze testowym są bardzo zbliżone. Minimalnie słabsze jest klasyfikowanie nie-napastników, stąd też lekki spadek dokładności."""

print(classification_report(y_test, svm_test_pred, zero_division=1))

cf_matrix = confusion_matrix(y_test, svm_test_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Other positions', 'Forwards'])

fig, ax = plt.subplots(figsize=(10,10))
cmd.plot(ax=ax, cmap='Purples')
plt.grid(False)
plt.show()

test_scores.append(svm_test_pred)

"""####Kernel wielomianowy.

Dla kernela wielomianowego pozostańmy konsekwentni względem kernela liniowego we wskazywaniu wartości parametru C, przetestujmy przy tym takie same, jak w przypadku regresji logistycznej, stopnie wielomianu.
"""

params = {'C': [0.01, 1, 2, 10, 25, 50, 100], 'degree': [1,2,3]}
svc = SVC(probability=True, kernel='poly')

gs = GridSearchCV(svc, param_grid=params, cv=10, verbose=10)
gs.fit(X_train_sc, y_train)
poly_svm_train_pred = gs.predict(X_train_sc)

"""Model zadziałał - jak na dużą liczbę iteracji - bardzo szybko. Co jest charakterystyczne dla maszyn wektorów nośnych, a co nie ujawniło się szczególnie przy kernelu liniowym - współczynnik C jest wysoki. Co ciekawe, w zasadzie nie możemy mówić o wielomianie, gdyż optymalny jest stopień pierwszy. Bardzo bliski poprzednich (nawet minimalnie wyższy) jest najlepszy wskazany wynik."""

print(gs.best_params_)
print(gs.best_estimator_)
print(gs.best_score_)

"""Jak można było przewidywać po wskazaniach GridSearcha, wyniki są znów zbliżone. Tak jak w poprzednich modelach, dokładność na zbiorze treningowym wynosi 0,93. Ten model popełnia jeden więcej niż kernel liniowy błąd we wskazaniu napastników."""

print(classification_report(y_train, poly_svm_train_pred, zero_division=1))

cf_matrix = confusion_matrix(y_train, poly_svm_train_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Other positions', 'Forwards'])

fig, ax = plt.subplots(figsize=(10,10))
cmd.plot(ax=ax)
plt.grid(False)
plt.show()

train_scores.append(poly_svm_train_pred)

poly_svm_test_pred = gs.predict(X_test_sc)

"""Przy dwukrotnie mniejszym zbiorze testowym błędy - szczególnie w zakresie wskazania nie-napastników - występują stosunkowo częściej. Tym niemniej również ten model należy uznać za stosunkowo udany."""

print(classification_report(y_test, poly_svm_test_pred, zero_division=1))

cf_matrix = confusion_matrix(y_test, poly_svm_test_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Other positions', 'Forwards'])

fig, ax = plt.subplots(figsize=(10,10))
cmd.plot(ax=ax, cmap='Purples')
plt.grid(False)
plt.show()

test_scores.append(poly_svm_test_pred)

"""###Metoda k-najbliższych sąsiadów.

Dla metody k-najbliższych sąsiadów dobieram nowy zestaw parametrów: n_neighbors, określający liczbę "sąsiadów" wykorzystanych do "głosowania" nad przypisaniem rekordu, oraz weights, wskazujący wagi sąsiednich punktów - czy jest jednolita, czy zależna od odległości (im bliższy punkt, tym wyższa waga).
"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier()
params = {'n_neighbors': [5, 11, 15, 25], 'weights': ['uniform', 'distance']}
gs = GridSearchCV(knn, param_grid=params, cv=10, verbose=10)
gs.fit(X_train_sc, y_train)
knn_train_pred = gs.predict(X_train_sc)

"""Model zadziałał bardzo szybko i wyliczył, że najlepiej sprawuje się przy 11 sąsiednich punktach i jednolitych wagach tychże. Problemem jednak jest niższy najlepszy wynik: spadł on na poziom 0.896."""

print(gs.best_params_)
print(gs.best_estimator_)
print(gs.best_score_)

"""Model niestety charakteryzuje się najsłabszą dokładnością w wykrywaniu napastników - stosunek wartości TP do FN to 230:78 w zbiorze treningowym i 104:50 w zbiorze testowym. Wprawdzie ponownie dobrze wykrywa on wartości TN (na zbiorze treningowym nawet najlepiej z dotychczasowych), jednakże dla wszystkich dotychczasowych modeli nie było to większym wyzwaniem."""

print(classification_report(y_train, knn_train_pred, zero_division=1))

cf_matrix = confusion_matrix(y_train, knn_train_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Other positions', 'Forwards'])

fig, ax = plt.subplots(figsize=(10,10))
cmd.plot(ax=ax)
plt.grid(False)
plt.show()

train_scores.append(knn_train_pred)

knn_test_pred = gs.predict(X_test_sc)

"""Nadto problemem jest znaczny overfitting - oprócz słabej klasyfikacji napastników, mamy do czynienia z wyraźnym spadkiem dokładności."""

print(classification_report(y_test, knn_test_pred, zero_division=1))

cf_matrix = confusion_matrix(y_test, knn_test_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Other positions', 'Forwards'])

fig, ax = plt.subplots(figsize=(10,10))
cmd.plot(ax=ax, cmap='Purples')
plt.grid(False)
plt.show()

test_scores.append(knn_test_pred)

"""###Naiwny klasyfikator Bayesa (NB).

####Gaussowski NB.

Dla modelu gaussowskiego, opartego na twierdzeniu Bayesa, dobieramy hiperparametr var_smoothing - wygładzanie wykładnicze.
"""

from sklearn.naive_bayes import GaussianNB

params = {'var_smoothing': np.linspace(0, 1, num=100)}
gnb = GaussianNB()
gs = GridSearchCV(gnb, param_grid=params, cv=10, verbose=10)
gs.fit(X_train_sc, y_train)
gnb_train_pred = gs.predict(X_train_sc)

"""Model działa bardzo szybko nawet przy wysokiej liczbie parametrów. GridSearchCV ustalił wysokość parametru wygładzania na poziomie ok. 0,02. Odnotować należy jednak spadek najlepszego wyniku na poziom 0,85."""

print(gs.best_params_)
print(gs.best_estimator_)
print(gs.best_score_)

"""Niestety, model jest szalenie niedokładny. O ile nie-napastnikami radzi sobie całkiem przyzwoicie (chociaż 74 błędne wskazania to zdecydowanie największa liczba jak dotąd), o tyle w stosunku do napastników notuje ponad 40% błędnych wskazań!"""

print(classification_report(y_train, gnb_train_pred, zero_division=1))

cf_matrix = confusion_matrix(y_train, gnb_train_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Other positions', 'Forwards'])

fig, ax = plt.subplots(figsize=(10,10))
cmd.plot(ax=ax)
plt.grid(False)
plt.show()

train_scores.append(gnb_train_pred)

gnb_test_pred = gs.predict(X_test_sc)

"""Nieco lepiej sytuacja przedstawia się na zbiorze testowym - można by rzec, że jest to chyba jedyny jak dotąd model, którego wyniki uległy drobnej poprawie. Nie zmienia to faktu, że jest najgorzej działający z dotychczas przedstawionych modeli."""

print(classification_report(y_test, gnb_test_pred, zero_division=1))

cf_matrix = confusion_matrix(y_test, gnb_test_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Other positions', 'Forwards'])

fig, ax = plt.subplots(figsize=(10,10))
cmd.plot(ax=ax, cmap='Purples')
plt.grid(False)
plt.show()

test_scores.append(gnb_test_pred)

"""####Wielomianowy NB.

Naive-Bayesowi wielomianowemu wskażemy jako hiperparametr współczynnik wygładzania addytywnego (Laplace'a/Lidstone'a), ukryty pod nazwą parametru alpha. Podobnie jak w przypadku wygładzania wykładniczego w gaussowskim NB, podamy mu do przetestowania jako wartość 100 liczb z zakresu od 0 do 1. Wielomianowy NB wykorzystuje dane niestandaryzowane.
"""

from sklearn.naive_bayes import MultinomialNB

params = {'alpha': np.linspace(0, 1, num=100)}
mnb = MultinomialNB()
gs = GridSearchCV(mnb, param_grid=params, cv=10, verbose=10)
gs.fit(X_train, y_train)
mnb_train_pred = gs.predict(X_train)

"""Co interesujące, z tak dużego zakresu GridSearchCV wybrał jako najbardziej optymalny alpha 0.0. Najlepszy wynik jest jednak jeszcze niższy niż w poprzednich wypadkach."""

print(gs.best_params_)
print(gs.best_estimator_)
print(gs.best_score_)

"""Ten model jest pierwszym jak dotąd tak skutecznym w klasyfikowaniu napastników (stosunek 299:9 przewidywań poprawnych do niepoprawnych), a zarazem tak fatalnie się sprawdzającym przy wychwytywaniu nie-napastników - w ich przypadku model myli się 5 razy częściej niż większość dotychczasowych modeli i ponad 3 razy częściej niż najsłabszy!"""

print(classification_report(y_train, mnb_train_pred, zero_division=1))

cf_matrix = confusion_matrix(y_train, mnb_train_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Other positions', 'Forwards'])

fig, ax = plt.subplots(figsize=(10,10))
cmd.plot(ax=ax)
plt.grid(False)
plt.show()

train_scores.append(mnb_train_pred)

mnb_test_pred = gs.predict(X_test)

"""Model nie tylko słabo dostosował się do danych, ale również jego wyniki spadły w praktycznie każdej metryce."""

print(classification_report(y_test, mnb_test_pred, zero_division=1))

cf_matrix = confusion_matrix(y_test, mnb_test_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Other positions', 'Forwards'])

fig, ax = plt.subplots(figsize=(10,10))
cmd.plot(ax=ax, cmap='Purples')
plt.grid(False)
plt.show()

test_scores.append(mnb_test_pred)

"""###Klasyfikator drzewa decyzyjnego.

Drzewu decyzyjnemu dobieramy jako hiperparametry: maksymalną głębokość, minimalną liczbę obserwacji w liściu potrzebną do podziału (min_samples_split) i minimalną wielkość liścia po podziale (min_samples_leaf). Również w tym wypadku nie korzystamy z ustandaryzowanych danych.
"""

from sklearn.tree import DecisionTreeClassifier

params = {'max_depth': [3,4,5], 'min_samples_split': [2,4,6], 'min_samples_leaf': [2,3,4,5]}
dtc = DecisionTreeClassifier()
gs = GridSearchCV(dtc, param_grid=params, cv=10, verbose=10)
gs.fit(X_train, y_train)
dtc_train_pred = gs.predict(X_train)

"""Optymalna maksymalna głębokość drzewka wynosi pięć, tak samo jak liczba obserwacji na liść, wielkość liścia natomiast GridSearch wyliczył na dwa. Wzrósł - w stosunkowo do modeli opartych na naiwnym Bayesie - estymowany wynik i wynosi on nieco ponad 0,9."""

print(gs.best_params_)
print(gs.best_estimator_)
print(gs.best_score_)

"""Skuteczność przewidywań drzewka jest wysoka, metryki na zbiorze treningowym z reguły przekraczają 0,9. Bardzo dobrze wygląda trafność przewidywań nie-napastników - raptem 34 błędy, powyżej przeciętnej jest również klasyfikowanie napastników.


"""

print(classification_report(y_train, dtc_train_pred, zero_division=1))

cf_matrix = confusion_matrix(y_train, dtc_train_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Other positions', 'Forwards'])

fig, ax = plt.subplots(figsize=(10,10))
cmd.plot(ax=ax)
plt.grid(False)
plt.show()

train_scores.append(dtc_train_pred)

dtc_test_pred = gs.predict(X_test)

"""Niestety, wyniki dla zbioru testowego wskazują na znaczny overfitting. Metryki wyraźnie spadły. Mimo dwukrotnie mniejszego zbioru testowego stosunek błędów w zbiorze testowym do błędów w treningowym wynosi: w stosunku do napastników - 2/3 (33:52), a w stosunku do pozostałych pozycji - niemal 9/10 (30:34)."""

print(classification_report(y_test, dtc_test_pred, zero_division=1))

cf_matrix = confusion_matrix(y_test, dtc_test_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Other positions', 'Forwards'])

fig, ax = plt.subplots(figsize=(10,10))
cmd.plot(ax=ax, cmap='Purples')
plt.grid(False)
plt.show()

test_scores.append(dtc_test_pred)

"""###Bagging.

Dla klasyfikatora lasu losowego, oprócz podanych przy drzewie decyzyjnym, dobieramy n_estimators: minimalną liczbę "drzew" w "lesie".
"""

from sklearn.ensemble import RandomForestClassifier

params = {'max_depth': [3,4,5], 'min_samples_split': [2,4,6], 'min_samples_leaf': [2,3,4,5], 'n_estimators': [25,50,100]}
rfc = RandomForestClassifier()
gs = GridSearchCV(rfc, param_grid=params, cv=10, verbose=10)
gs.fit(X_train, y_train)
rfc_train_pred = gs.predict(X_train)

"""Głębokość drzewek powinna być wg GridSearcha dość duża, bo aż pięć, a na las decyzyjny składa się ich aż 100. Dobrze wygląda zaś estymowany wynik: ponownie osiągnął on pułap ok. 0,92."""

print(gs.best_params_)
print(gs.best_estimator_)
print(gs.best_score_)

"""Model świetnie wychwycił zawodników niewystępujących jako napastnicy - w zbiorze treningowym pomylił się ledwie 20 razy! Niestety, ma on znacznie słabszą skuteczność w odsiewaniu napastników. Przeciętna dokładność w granicach 0,95 wygląda ciekawie, jednak - mając na uwadze przeciętny wynik - zwiastuje pewien overfitting."""

print(classification_report(y_train, rfc_train_pred, zero_division=1))

cf_matrix = confusion_matrix(y_train, rfc_train_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Other positions', 'Forwards'])

fig, ax = plt.subplots(figsize=(10,10))
cmd.plot(ax=ax)
plt.grid(False)
plt.show()

train_scores.append(rfc_train_pred)

rfc_test_pred = gs.predict(X_test)

"""Niestety, i ten model charakteryzuje się overfittingiem. Wszystkie wyniki spadają, a błędów w przewidywaniu nie-napastników jest nawet o 2 więcej niż w zbiorze uczącym. Mimo względnie wysokiego wyniku, jest to dlań dyskwalifikujące."""

print(classification_report(y_test, rfc_test_pred, zero_division=1))

cf_matrix = confusion_matrix(y_test, rfc_test_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Other positions', 'Forwards'])

fig, ax = plt.subplots(figsize=(10,10))
cmd.plot(ax=ax, cmap='Purples')
plt.grid(False)
plt.show()

test_scores.append(rfc_test_pred)

"""###Boosting.

AdaBoostClassifierowi, jako zespołowi klasyfikatorów, wskażę jako parametry liczbę klasyfikatorów w zespole (n_estimators), wagi nadawane klasyfikatorom przy każdej kolejnej iteracji (learning_rate) oraz algorytm boostingu (SAMME - boosting dyskretny, SAMME.R - boosting realny). Tablica zbudowana do iteracji dla drugiego z parametrów będzie nieco mniejsza i składać się będzie z 15 elementów, aby model zadziałał względnie szybko, a minimalna wartość znajdująca się w niej będzie większa od 0 (maksymalna to 1), bo przy zerowej wadze klasyfikatorów GridSearchCV wskazuje wynik NaN. W różnych modelach prezentuje się różne podejścia do wykorzystania zestandaryzowanych danych przy boostingu - ja jednak uważam, że warto wykorzystywać standaryzację, zwłaszcza gdy wcześniej zastosowano redukcję wymiarowości w celu zmniejszenia zbioru cech.
"""

from sklearn.ensemble import AdaBoostClassifier

params = {'n_estimators': [75, 100], 'learning_rate': np.linspace(0.001, 1, num=15), 'algorithm': ['SAMME', 'SAMME.R']}
abc = AdaBoostClassifier()
gs = GridSearchCV(abc, param_grid=params, cv=10, verbose=10)
gs.fit(X_train_sc, y_train)
abc_train_pred = gs.predict(X_train_sc)

"""Algorytm nie należy do przesadnie szybkich, ale daje obiecujące wyniki: 100 klasyfikatorów i ich wagi na poziomie ok. 0.786 oraz algorytm SAMME - to optymalne parametry dobrane przez GridSearcha. Najlepszy średni wynik dla iteracji jest obiecujący i wynosi ok. 0.925."""

print(gs.best_params_)
print(gs.best_estimator_)
print(gs.best_score_)

"""Dokładność tego modelu wobec zbioru treningowego jest wysoka - wynosi 0.94. Co ciekawe, jego wyniki stanowią kompromis między wysoką dokładnością klasyfikowania napastników i pozostałych graczy - w przypadku tych pierwszych myli się 44 razy na 308 obserwacji, w przypadku drugich - 35 razy na 1112 obserwacji. Jak dobrze jednak wiadomo, wartość modelu oceni nam jego skuteczność na zbiorze testowym."""

print(classification_report(y_train, abc_train_pred, zero_division=1))

cf_matrix = confusion_matrix(y_train, abc_train_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Other positions', 'Forwards'])

fig, ax = plt.subplots(figsize=(10,10))
cmd.plot(ax=ax)
plt.grid(False)
plt.show()

train_scores.append(abc_train_pred)

abc_test_pred = gs.predict(X_test_sc)

"""Niestety, model okazuje się być przeuczony. Dokładność spadła z 0.94 na 0.91, przy czym stosunkowo model myli się wyraźnie częściej. Liczba błędów spadła tylko o kilka wyników w obu klasach."""

print(classification_report(y_test, abc_test_pred, zero_division=1))

cf_matrix = confusion_matrix(y_test, abc_test_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Other positions', 'Forwards'])

fig, ax = plt.subplots(figsize=(10,10))
cmd.plot(ax=ax, cmap='Purples')
plt.grid(False)
plt.show()

test_scores.append(abc_test_pred)

"""###Sieć neuronowa - MLP Classifier.

Scikit-learn posiada własny moduł pozwalający na klasyfikację przy użyciu sztucznych sieci neuronowych. Z uwagi na długi czas iteracji, ograniczmy się do dwóch hiperparametrów:
- hidden_layer_sizes: krotka określająca liczbę warstw ukrytych perceptronu i zawartych w nich neuronów, 
- activation: funkcja aktywacji neuronu wyjścia,
- solver" algorytm optymalizacji.

Sieć będzie w obu przypadkach składać się z tej samej liczby neuronów: 50, rozmieszczonych w różnych konfiguracjach.
"""

from sklearn.neural_network import MLPClassifier

params ={
    'hidden_layer_sizes': [(20,20,10), (10,10,10,10,10)],
    'activation': ['tanh', 'relu', 'identity', 'logistic'],
    'solver': ['adam', 'sgd']
}

mlp = MLPClassifier(max_iter=50000)
gs = GridSearchCV(mlp, param_grid=params, cv=10, verbose=10)
gs.fit(X_train_sc, y_train)
mlp_train_pred = gs.predict(X_train_sc)

"""Model, niestety, nie należy do najszybszych - na niektóre iteracje potrzebuje nawet kilkanaście sekund. GridSearch wskazuje, że optymalną funkcją aktywacji jest funkcja tożsamościowa, optymalizatorem - ADAM (Adaptive Moment Estimation), zaś najlepiej działa sieć składająca się z 5 warstw ukrytych z 10 neuronami każda. Wynik ok. 0,925 wygląda przyzwoicie, jednak przy takim koszcie obliczeń nie jest on zbyt zadowalający."""

print(gs.best_params_)
print(gs.best_estimator_)
print(gs.best_score_)

"""Sieć notuje przyzwoite wyniki na zbiorze treningowym. Dokładność wyniosła 0,93. Tradycyjnie już model dobrze wykrywa zawodników z innych pozycji niż napastnicy (1061/1112 dokładności) i słabiej napastników (258/308)."""

print(classification_report(y_train, mlp_train_pred, zero_division=1))

cf_matrix = confusion_matrix(y_train, mlp_train_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Other positions', 'Forwards'])

fig, ax = plt.subplots(figsize=(10,10))
cmd.plot(ax=ax)
plt.grid(False)
plt.show()

train_scores.append(mlp_train_pred)

mlp_test_pred = gs.predict(X_test_sc)

"""Niestety, wyniki na zbiorze testowym są minimalnie gorsze. Spada dokładność przewidywania w przypadku obu kategorii."""

print(classification_report(y_test, mlp_test_pred, zero_division=1))

cf_matrix = confusion_matrix(y_test, mlp_test_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Other positions', 'Forwards'])

fig, ax = plt.subplots(figsize=(10,10))
cmd.plot(ax=ax, cmap='Purples')
plt.grid(False)
plt.show()

test_scores.append(mlp_test_pred)

"""##Podsumowanie.

Zbiór danych, który wykorzystałem w tej części projektu, nie należy do najłatwiejszych w budowaniu na jego podstawie modelu. Modele z reguły bardzo dobrze radziły sobie w klasyfikowaniu zawodników niegrających jako napastnicy i z reguły dokładność w przewidywaniu tychże przekracza 0,95. Gorzej jest z klasyfikowaniem napastników - najlepsze modele mają w tym zakresie dokładność w granicach 0,8. Bardzo dobrą skuteczność w tym zakresie notował wielomianowy Naive-Bayes, jednakowoż bardzo źle spisywał się on w klasyfikowaniu większej grupy.

Problemem w klasyfikacji napastników może być natomiast fakt, iż część z nich odnotowuje słabe statystyki - zdobywają mało goli, zaliczają niewiele asyst, rzadziej próbują dryblować. Da się to uzasadnić ich słabszymi umiejętnościami, grą w słabych zespołach, które nie tworzą im dogodnych sytuacji na stworzenie zagrożenia, względnie rolą na boisku - często nominalni napastnicy pełnią w istocie funkcję zawodnika, którego zadaniem jest absorbować na sobie obrońców, odgrywać piłkę do lepiej ustawionych partnerów. Są też sytuacje przeciwne - nominalni pomocnicy, a rzadziej obrońcy, notują tak dobre statystyki w ofensywie, że modele klasyfikują ich jako napastników. Są to jednak wyjątki od ogólnej reguły, co znajduje potwierdzenie w wynikach uwidocznionych przez macierze pomyłek.

Wracając do samego modelu - porównajmy sobie wyniki dokładności poszczególnych modeli i sprawdźmy raz jeszcze, które są najdokładniejsze, które gwarantują najmniejszy spadek skuteczności w zbiorze testowym itp.

Wykorzystujemy w tym zbudowane wcześniej listy, które wcześniej konsekwentnie budowaliśmy, i na ich podstawie stwórzmy słowniki, które jako wartości przyjmą metryki dokładności i F1-score zaś jako klucze - wskazanie, których zbiorów dotyczą.
"""

scores = {'Training accuracy': [accuracy_score(y_train, i) for i in train_scores],
          'Test accuracy': [accuracy_score(y_test, i) for i in test_scores],
          'Training F1 score': [f1_score(y_train, i) for i in train_scores],
          'Test F1 score': [f1_score(y_test, i) for i in test_scores]}

"""Taka konstrukcja słowników pozwoli nam łatwo przekształcić je w kolumny ramki danych. Jako indeksy wskażemy nazwy poszczególnych klasyfikatorów, do których wyniki się odnoszą."""

scores = pd.DataFrame(scores, index=['Logistic Regression', 'SVM', 'Polynomial SVM',
                                     'K-nearest neighbors', 'Gaussian NB', 
                                     'Multinomial NB', 'Decision tree',
                                     'Bagging', 'Boosting', 'MLP'])
scores

"""Stwórzmy kolumny, która uwidocznią nam różnice między wynikami ze zbioru treningowego i testowego."""

scores['Accuracy difference'] = scores['Training accuracy'] - scores['Test accuracy']
scores['F1 difference'] = scores['Training F1 score'] - scores['Test F1 score']
scores

"""Wnioski:
- najlepiej z klasyfikacją poradziły sobie Support Vectors Machines; kernele liniowy i wielomianowy notują bardzo zbliżone wyniki na obu zbiorach, w tym najwyższe na zbiorze testowym, ponadto są szybkie obliczeniowo. Zbliżone wyniki notują: regresja logistyczna, która jednak działa powoli w przypadku dopasowania stopnia wielomianu wyższego niż drugi, i sieci neuronowe, które w każdym wypadku działają za wolno,
- zespoły klasyfikatorów (Random Forest i AdaBoost), choć wykazują wysoką skuteczność na zbiorze treningowym, są przeuczone, stosunkowo częściej mylą się w zbiorze testowym niż w uczącym i odnotowują znaczne spadki zarówno, jeśli chodzi o dokładność, jak i F1 score,
- rozczarowująco działa klasyfikator oparty na sztucznych sieciach neuronowych - jego wyniki nie przedstawiają się najgorzej, ale dyskwalifikuje go słaba prędkość działania,
- klasyfikatory oparte na klasyfikatorze naiwnego Bayesa wypadają zdecydowanie najsłabiej, co może w jakiejś części wynikać z różnicy w liczebności klas, jak i z samych założeń, na których opiera się ich działanie,
- klasyfikatory wykorzystujące koncepcje drzew/lasów decyzyjnych, zespoły klasyfikatorów i metoda k-najbliższych sąsiadów mają największy overfitting,
- jedynym klasyfikatorem z lepszym wynikiem w zbiorze testowym jest gaussowski Naive Bayes, jednak problemy z nim nie pozwalają uznać go za dobrej jakości klasyfikator. Przyjmując jako dopuszczalną granicę między dobrym wyuczeniem a przeuczeniem różnicę 0,01 w dokładności i 0,02 w metryce F1 na korzyść zbioru treningowego, widzimy, że z dobrze działających klasyfikatorów w tej kategorii mieszczą się jedynie SVM-y i sieci neuronowe.

#Kategoryzacja wieloklasowa.

Przechodzimy do klasyfikacji wieloklasowej. Tym razem nasze modele będą miały za zadanie przypisać wszystkich zawodników do każdej z pozycji - bramkarz, obrońca, pomocnik, napastnik. Odpuszczę sobie klasyfikatory drzewa decyzyjnego i K-najbliższych sąsiadów (ze względu na niewyróżniające się niczym wyniki), wielomianowego Naive Bayesa (gdyż w zupełności wystarczy sprawdzenie gaussowskiego dla zbadania skuteczności modeli opartych na tym twierdzeniu) i sztuczne sieci neuronowe (gdyż klasyfikacja z ich wykorzystaniem może trwać bardzo długo).

Zaczynamy od wykorzystania LabelEncodera, który przekształci poszczególne kategorie typu string na etykiety numeryczne.
"""

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y_l = le.fit_transform(Y)

le.classes_

np.unique(y_l)

"""Proporcje zbiorów treningowego i testowego zachowujemy takie, jak w przypadku pierwszej części projektu."""

X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y_l,
                                                    test_size = 0.333,
                                                    random_state = 42, stratify=y_l)

"""Ponownie standaryzujemy zbiory treningowy i testowy, dokonujemy również redukcji wymiarowości z użyciem tej samej liczby cech, co w klasyfikacji binarnej."""

sc = StandardScaler()
X_train_sc = sc.fit_transform(X_train)
X_test_sc = sc.transform(X_test)

pca = PCA(n_components=20)
X_train_sc = pca.fit_transform(X_train_sc)
X_test_sc = pca.transform(X_test_sc)

"""##Regresja logistyczna.

Z uwagi na długość obliczeń (nawet kilkanaście sekund na iterację), ograniczmy liczbę testowanych stopni wielomianu do pierwszego i drugiego. W zamian za to wypróbuję więcej wartości współczynnika kary C, jednak tym razem (z ww. powodów) mniejszych.
"""

pipe = Pipeline([
    ('poly', PolynomialFeatures()),
    ('lr', LogisticRegression(warm_start=True, max_iter=50000))
])

params = {'poly__degree': [1, 2], 'lr__C': [0.01, 0.1, 1, 2, 5, 10]}
gs = GridSearchCV(pipe, param_grid=params, cv=10, verbose=10)
gs.fit(X_train_sc, y_train)
lr_train_pred = gs.predict(X_train_sc)

"""Dobór parametrów trwał dość długo, niektóre iteracje trwały niemal tyle, ile przy klasyfikacji z użyciem sieci neuronowych. Optymalne parametry to: pierwszy stopień wielomianu i współczynnik C równy 5. Estymowany wynik jest znacznie niższego rzędu niż w wypadku poprzedniego modelowania i oscyluje w granicach 0,794."""

print(gs.best_params_)
print(gs.best_estimator_)
print(gs.best_score_)

"""Model notuje 100% skuteczności przy "odsiewaniu" bramkarzy, jednak wyraźnie gorzej spisuje się wobec pozostałych kategorii. Na uwagę zasługuje słaba skuteczność w stosunku do pomocników - wszystkie metryki przyjmują wartość ok. 0,72."""

print(classification_report(le.inverse_transform(y_train), le.inverse_transform(lr_train_pred), zero_division=1))

"""Z macierzy pomyłek wynika, że żadna z pozycji nie jest mylona z bramkarzami. Model nawet lepiej niż w klasyfikacji binarnej poradził sobie z napastnikami. Duże trudności napotkała klasyfikacja pomocników - myleni są stosunkowo często zarówno z obrońcami (43 razy), jak i z napastnikami (95 razy)."""

cf_matrix = confusion_matrix(y_train, lr_train_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Attackers', 'Defenders', 'Goalkeepers', 'Midfielders'])

fig, ax = plt.subplots(figsize=(12,8))
cmd.plot(ax=ax)
plt.grid(False)
plt.show()

lr_test_pred = gs.predict(X_test_sc)

"""Ewalucja na poziomie zbioru testowego wskazuje jednak na znaczny overfitting. Wyniki spadają - poza bramkarzami - w każdej możliwej rubryce. Bezprecedensowy w warunkach projektu jest spadek dokładności - o 0,05!"""

print(classification_report(le.inverse_transform(y_test), le.inverse_transform(lr_test_pred), zero_division=1))

"""Model ponownie nie radzi sobie szczególnie dobrze z pomocnikami: na 247 błędnie wskazał 84."""

cf_matrix = confusion_matrix(y_test, lr_test_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Attackers', 'Defenders', 'Goalkeepers', 'Midfielders'])

fig, ax = plt.subplots(figsize=(12,8))
cmd.plot(ax=ax)
plt.grid(False)
plt.show()

"""Wzorem części z klasyfikacją binarną, tworzymy listy celem dalszego przekształcania."""

train_scores = []
train_scores.append(lr_train_pred)

test_scores = []
test_scores.append(lr_test_pred)

"""##SVM.

###Kernele jednomianowe.

Dla kernela jednomianowego ponownie dobieramy te same wartości parametrów, co przy poprzedniej sposobności.
"""

params = {'C': [0.01, 1, 2, 10, 25, 50, 100], 'kernel': ['linear', 'rbf', 'sigmoid']}
svc = SVC()
gs = GridSearchCV(svc, param_grid=params, cv=10, verbose=10)
gs.fit(X_train_sc, y_train)
svm_train_pred = gs.predict(X_train_sc)

"""Również tutaj wyniki są przyzwoite - przy przyjęciu kernela liniowego i współczynnika C = 50 estymowany wynik to ok. 0,8. Należy pamiętać, że klasyfikacja wieloklasowa w tym wypadku nie należy do najłatwiejszych - część rekordów na poszczególnych pozycjach jest do siebie dosyć podobna i często nie jest trudno o błąd."""

print(gs.best_params_)
print(gs.best_estimator_)
print(gs.best_score_)

"""100% skuteczności w stosunku do bramkarzy, słabsza wobec obrońców, nieco wyższa w przypadku pozostałych pozycji - ten model jest na zbiorze treningowym skuteczniejszy niż poprzedni."""

print(classification_report(le.inverse_transform(y_train), le.inverse_transform(svm_train_pred), zero_division=1))

"""Wyniki na macierzy pomyłek wskazują na całkiem sporą poprawę (odpowiednio 12 i 9) w przewidywaniu pomocników i napastników."""

cf_matrix = confusion_matrix(y_train, svm_train_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Attackers', 'Defenders', 'Goalkeepers', 'Midfielders'])

fig, ax = plt.subplots(figsize=(12,8))
cmd.plot(ax=ax)
plt.grid(False)
plt.show()

svm_test_pred = gs.predict(X_test_sc)

"""Niestety, znowu zachodzi przeuczenie. Dokładność modelu spada o 0,07."""

print(classification_report(le.inverse_transform(y_test), le.inverse_transform(svm_test_pred), zero_division=1))

cf_matrix = confusion_matrix(y_test, svm_test_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Attackers', 'Defenders', 'Goalkeepers', 'Midfielders'])

fig, ax = plt.subplots(figsize=(12,8))
cmd.plot(ax=ax)
plt.grid(False)
plt.show()

train_scores.append(svm_train_pred)

test_scores.append(svm_test_pred)

"""###Kernel wielomianowy.

Jak wyglądać będzie klasyfikacja przy przetestowaniu tych samych parametrów kernela wielomianowego?
"""

params = {'C': [0.01, 1, 2, 10, 25, 50, 100], 'degree': [1,2,3]}
svc = SVC(probability=True, kernel='poly')

gs = GridSearchCV(svc, param_grid=params, cv=10, verbose=10)
gs.fit(X_train_sc, y_train)
poly_svm_train_pred = gs.predict(X_train_sc)

"""Współczynnik C równy 25, pierwszy stopień i podobny wynik estymowany do poprzednich klasyfikatorów - tak przewiduje GridSearchCV."""

print(gs.best_params_)
print(gs.best_estimator_)
print(gs.best_score_)

"""Tak jak w poprzednich wypadkach, większych różnic brak. Uwagi dot. poprzednich modeli w całości można podtrzymać. Odchylenia w poszczególnych rekordach pozostają w granicach błędu statystycznego."""

print(classification_report(le.inverse_transform(y_train), le.inverse_transform(poly_svm_train_pred), zero_division=1))

cf_matrix = confusion_matrix(y_train, poly_svm_train_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Attackers', 'Defenders', 'Goalkeepers', 'Midfielders'])

fig, ax = plt.subplots(figsize=(12,8))
cmd.plot(ax=ax)
plt.grid(False)
plt.show()

poly_svm_test_pred = gs.predict(X_test_sc)

"""Niestety, również bolączką tego modelu jest overfitting. Wszystkie metryki odnotowały wyraźny spadek, modelowi zdarzyło się również błędne przypisanie bramkarza jako napastnika."""

print(classification_report(le.inverse_transform(y_test), le.inverse_transform(poly_svm_test_pred), zero_division=1))

cf_matrix = confusion_matrix(y_test, poly_svm_test_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Attackers', 'Defenders', 'Goalkeepers', 'Midfielders'])

fig, ax = plt.subplots(figsize=(12,8))
cmd.plot(ax=ax)
plt.grid(False)
plt.show()

train_scores.append(poly_svm_train_pred)

test_scores.append(poly_svm_test_pred)

"""##Gaussowski NB.

Ponownie GridSearchCV dobierze dla gaussowskiego NB parametr wygładzania.
"""

params = {'var_smoothing': np.linspace(0, 1, num=100)}
gnb = GaussianNB()
gs = GridSearchCV(gnb, param_grid=params, cv=10, verbose=10)
gs.fit(X_train_sc, y_train)
gnb_train_pred = gs.predict(X_train_sc)

"""Od samej wartości parametru bardziej interesujący jest estymowany wynik - wynosi bowiem poniżej 0,6."""

print(gs.best_params_)
print(gs.best_estimator_)
print(gs.best_score_)

"""Jest to pierwszy (i zapewne jedyny) model, który tak drastycznie myli się w przewidywaniach. Spadek widoczny jest w każdej kategorii, nawet w stosunku do bramkarzy (1/3 błędnych przewidywań), model nie radzi sobie kompletnie z napastnikami, których poprawnie przypisał zaledwie 41/308."""

print(classification_report(le.inverse_transform(y_train), le.inverse_transform(gnb_train_pred), zero_division=1))

cf_matrix = confusion_matrix(y_train, gnb_train_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Attackers', 'Defenders', 'Goalkeepers', 'Midfielders'])

fig, ax = plt.subplots(figsize=(12,8))
cmd.plot(ax=ax)
plt.grid(False)
plt.show()

gnb_test_pred = gs.predict(X_test_sc)

"""Powyższa obserwacja uwydatnia się na zbiorze testowym. Przewidywanie bramkarzy daje rezultaty zbliżone do przewidywania na podstawie rzutu monetą."""

print(classification_report(le.inverse_transform(y_test), le.inverse_transform(gnb_test_pred), zero_division=1))

cf_matrix = confusion_matrix(y_test, gnb_test_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Attackers', 'Defenders', 'Goalkeepers', 'Midfielders'])

fig, ax = plt.subplots(figsize=(12,8))
cmd.plot(ax=ax)
plt.grid(False)
plt.show()

train_scores.append(gnb_train_pred)

test_scores.append(gnb_test_pred)

"""##Bagging.

Również lasowi losowemu przypiszemy podobne wartości, co w przypadku klasyfikacji binarnej.
"""

params = {'max_depth': [3,4,5], 'min_samples_split': [2,4,6], 'min_samples_leaf': [2,3,4,5], 'n_estimators': [25,50,100]}
rfc = RandomForestClassifier()
gs = GridSearchCV(rfc, param_grid=params, cv=10, verbose=10)
gs.fit(X_train, y_train)
rfc_train_pred = gs.predict(X_train)

"""Wyniki są słabsze od poprzednich obserwacji (ok. 0,77), na co składają się: głębokość drzewa: 5, minimalna wielkość liścia: 3, liczba obserwacji na liść: 2, a także minimalna liczba drzew w lesie: 100."""

print(gs.best_params_)
print(gs.best_estimator_)
print(gs.best_score_)

"""Na zbiorze testowym wyniki wyglądają jak dotąd najlepiej. Średnio ok. 3/4 pomocników została zaklasyfikowana poprawnie, co jest wynikiem powyżej średniej."""

print(classification_report(le.inverse_transform(y_train), le.inverse_transform(rfc_train_pred), zero_division=1))

cf_matrix = confusion_matrix(y_train, rfc_train_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Attackers', 'Defenders', 'Goalkeepers', 'Midfielders'])

fig, ax = plt.subplots(figsize=(12,8))
cmd.plot(ax=ax)
plt.grid(False)
plt.show()

rfc_test_pred = gs.predict(X_test)

"""Niestety, okazało się, że model zmaga się ze znacznym overfittingiem. Dokładność spadła o 0,09 do poziomu 0,73, wszystkie metryki mają niższe wartości, nawet ta dotycząca bramkarzy."""

print(classification_report(le.inverse_transform(y_test), le.inverse_transform(rfc_test_pred), zero_division=1))

cf_matrix = confusion_matrix(y_test, rfc_test_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Attackers', 'Defenders', 'Goalkeepers', 'Midfielders'])

fig, ax = plt.subplots(figsize=(12,8))
cmd.plot(ax=ax)
plt.grid(False)
plt.show()

train_scores.append(rfc_train_pred)

test_scores.append(rfc_test_pred)

"""##Boosting.

Kończymy modelowanie, wykorzystując AdaBoostClassifiera z tymi samymi parametrami, co w poprzednim jego zastosowaniu.
"""

params = {'n_estimators': [75, 100], 'learning_rate': np.linspace(0.001, 1, num=15), 'algorithm': ['SAMME', 'SAMME.R']}
abc = AdaBoostClassifier()
gs = GridSearchCV(abc, param_grid=params, cv=10, verbose=10)
gs.fit(X_train_sc, y_train)
abc_train_pred = gs.predict(X_train_sc)

"""Wynik rzędu 0,7 nie zwiastuje modelu wysokiej jakości."""

print(gs.best_params_)
print(gs.best_estimator_)
print(gs.best_score_)

"""Okazuje się, że AdaBoostClassifier faktycznie spisuje się poniżej dotychczasowego standardu. Nie są to wyniki drastycznie słabe, ale - w stosunku do pozostałych klasyfikatorów - słabsze o ok. 0,1."""

print(classification_report(le.inverse_transform(y_train), le.inverse_transform(abc_train_pred), zero_division=1))

cf_matrix = confusion_matrix(y_train, abc_train_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Attackers', 'Defenders', 'Goalkeepers', 'Midfielders'])

fig, ax = plt.subplots(figsize=(12,8))
cmd.plot(ax=ax)
plt.grid(False)
plt.show()

abc_test_pred = gs.predict(X_test_sc)

"""Jeszcze słabiej jest w zbiorze testowym - model 2 razy błędnie przypisał bramkarzy. Podobne do zbioru treningowego zależności zachodzą w przewidywaniu pomocników i obrońców, wyraźnie niższe metryki mają przewidywania napastników."""

print(classification_report(le.inverse_transform(y_test), le.inverse_transform(abc_test_pred), zero_division=1))

cf_matrix = confusion_matrix(y_test, abc_test_pred)
cmd = ConfusionMatrixDisplay(cf_matrix, display_labels=['Attackers', 'Defenders', 'Goalkeepers', 'Midfielders'])

fig, ax = plt.subplots(figsize=(12,8))
cmd.plot(ax=ax)
plt.grid(False)
plt.show()

train_scores.append(abc_train_pred)

test_scores.append(abc_test_pred)

"""##Podsumowanie.

Klasyfikacja wieloklasowa poszła niestety słabiej. Najlepsze modele przekraczały dokładność 0,8 na zbiorze treningowym i oscylowały w okolicach 0,75 na zbiorze testowym. Dla poszczególnych klasyfikatorów (z wyjątkiem gaussowskiego NB) nie było wyzwaniem dopasowanie bramkarzy, natomiast żaden nie radził sobie wystarczająco dobrze z pomocnikami. Można zrzucić to na karb specyfiki tej pozycji, której przedstawiciele co do zasady operują między obroną a atakiem, a często są im przypisane zadania wsparcia którejś z tych dwóch formacji.

Ponownie zbieramy listy wszystkich modeli (z obu zbiorów), wyliczamy metryki dokładności i F1 score, a następnie dalej przekształcamy celem porównania
"""

scores = {'Training accuracy': [accuracy_score(y_train, i) for i in train_scores],
          'Test accuracy': [accuracy_score(y_test, i) for i in test_scores],
          'Training F1 score': [f1_score(y_train, i, average='weighted') for i in train_scores],
          'Test F1 score': [f1_score(y_test, i, average='weighted') for i in test_scores]}

scores = pd.DataFrame(scores, index=['Logistic Regression', 'SVM', 'Polynomial SVM',
                                     'Gaussian NB', 'Bagging', 'Boosting'])
scores['Accuracy difference'] = scores['Training accuracy'] - scores['Test accuracy']
scores['F1 difference'] = scores['Training F1 score'] - scores['Test F1 score']
scores

"""Wnioski:
- każdy z modeli zmaga się z problemem dopasowania do zbioru testowego. Najmniejsza różnica zachodzi w przypadku boostingu i gaussowskiego NB, jednakże klasyfikatory te są szalenie niedokładne,
- największym przeuczeniem charakteryzuje się bagging - pomimo najwyższego wyniku w zbiorze treningowym ma on - wskutek spadku metryki o niemal 0,09 punktu - jeden ze słabszych wyników na zbiorze testowym,
- ponownie najlepiej trzeba ocenić kernele; działają one szybko i - mimo istotnego spadku metryk - notują najlepsze rezultaty w zbiorze testowym. Podobne wyniki notuje regresja logistyczna, jednak jej duże wymagania obliczeniowe nie pozwalają jej traktować jako pierwszy wybór,
- potwierdza się słaba skuteczność gaussowskiego NB - modele bayesowskie nie wychwytują zależności między danymi, stąd ich wykorzystanie przy tak dużym zbiorze danych numerycznych może mieć wyłącznie charakter edukacyjny,
- dla każdego niemal klasyfikatora (z wyjątkiem gaussowskiego NB) dokładność i metryka F1 mają podobne wartości. W wypadku klasyfikacji wieloklasowej można rozważyć naprzemienne stosowanie tych metryk dla określenia jakości modelu.
"""